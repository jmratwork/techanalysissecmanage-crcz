# Training Materials and Workflows

This document outlines the theoretical training materials and the workflow expectations for both trainees and instructors operating within the KYPO cyber range environment. The primary scenario focuses on penetration testing and vulnerability assessment training using a Cyber Range environment that mirrors CYNET's network. Participants learn how to discover and document vulnerabilities while following organizational procedures. For operational alert triage in NG‑SOAR tools, refer to the [SOC Analyst Playbook](soc_analyst_playbook.md).

## Theoretical Background

Trainees should familiarize themselves with fundamental concepts in network security, incident response, and malware analysis prior to beginning exercises. Recommended topics include:

- Fundamentals of TCP/IP networking
- Common attack vectors and the kill chain methodology
- Basics of log analysis and threat intelligence
- Overview of vulnerability assessment and penetration testing methodologies

## Registration

Trainees are invited and registered through the training platform CLI in
`subcase_1b/training_platform/cli.py`. The script contacts the Open edX
service via `open_edx_client.py` to create course entries and confirm the
enrollment. Instructors typically bootstrap the platform with
`subcase_1b/scripts/training_platform_start.sh`, which starts the API that
handles these enrollment requests.

## Lab run

Once enrolled, trainees launch the hands‑on lab in the KYPO cyber range.
The environment is provisioned using `subcase_1b/scripts/cyber_range_start.sh`
and individualized scans can be executed with
`subcase_1b/scripts/trainee_start.sh --target <ip>`. Scenario specifics are
described in `subcase_1b/scenario.yml` and the corresponding topology file
`sandboxes/topology_subcase_1b.yaml`.

## Evaluation

Exercise results are submitted to the new results module implemented in
`subcase_1b/training_platform/results_service.py`. The service appends
entries to `results.json` and relays progress back to Open edX using the
same `open_edx_client.py` helper so that learner dashboards reflect the
outcome of the lab.

## Trainee Workflow

1. **Scenario Preparation** – Review the scenario description and objectives. Ensure access to required accounts and tools within CyberRangeCZ.
2. **Hands-on Investigation** – Use the training platform to follow course instructions and run semi-automated penetration tests against the Cyber Range.
3. **Reporting** – Compile findings into an assessment report, highlighting discovered vulnerabilities, applicable policy references, and suggested mitigations.

## Instructor Workflow

1. **Monitoring** – Ensure the Cyber Range and training platform are functioning and collect trainee reports.
2. **Evaluation** – Review results, correlate findings where necessary, and provide feedback or remediation guidance.

### Evaluation Flow Integration

Both trainees and instructors can submit exercise outcomes through the
training platform's `POST /results` endpoint. The service stores metrics
such as completion time and quiz scores in `results.json`, updates local
course progress, and relays that progress to the Open edX
`/courseware/` API so that learner dashboards show the latest status.

### API Usage

#### Trainees

- `POST /register` – create a trainee account.
- `POST /login` – exchange credentials for an authentication token.
- `POST /progress` / `GET /progress` – submit or fetch course progress.
- `POST /results` – manually upload lab scores and timing data.
- `POST /listener` – used by the KYPO range to push automatic score/flag
  updates to the platform.

#### Instructors

- `POST /courses` – create a new course shell.
- `POST /invites` – generate invite codes for trainees.
- `GET /courses` – list existing courses and metadata.
- `POST /results` – record evaluation outcomes for a trainee.

## Subcase 1c: Malware Handling

For detailed deployment, attack simulation, and validation procedures see the
[Subcase 1c guide](subcase_1c_guide.md).

### Trainee Activities

1. **Lab Deployment** – Start the Subcase 1c lab in the KYPO interface. *Validation:* confirm all virtual machines show a **running** state in the platform dashboard. *Artifacts:* screenshot of the deployment status and exported topology details.
2. **Run Malware Simulator** – Execute the provided malware simulator script inside the designated victim machine. *Validation:* the simulator must output a "simulation completed" message and create traffic logs on the host. *Artifacts:* terminal output log and generated host log files.
3. **Observation in NG-SIEM/NG-SOAR** – Monitor the NG-SIEM/NG-SOAR consoles for alerts generated by the simulator traffic. *Validation:* expected alert identifiers appear in the console with correlated events. *Artifacts:* exported alert report or screenshots of the correlated events.
4. **CTI Ingestion** – Ingest the threat indicators produced by the simulator into the CTI module. *Validation:* the ingestion log shows the indicators accepted and mapped to corresponding alerts. *Artifacts:* CTI feed file and ingestion confirmation log.
5. **Incident Escalation** – After validating an alert, escalate it to DFIR using `scripts/escalate_incident.sh <incident_id> <summary> [severity]`. The script posts the incident to the Decide service, which forwards it to CICMS/DFIR.
6. **Final Evaluation** – Submit findings through the platform’s evaluation form. *Validation:* the platform marks the exercise as completed and records the submission. *Artifacts:* final assessment report and platform submission receipt.

### Instructor Activities

1. **Lab Deployment** – Verify each trainee’s lab deployment using the KYPO control interface. *Validation:* orchestrator logs show successful provisioning of all machines. *Artifacts:* deployment logs and metrics exported from the interface.
2. **Run Malware Simulator** – Monitor simulator execution via remote console or log streaming. *Validation:* simulator run completes without errors and produces expected network activity. *Artifacts:* captured console output and packet capture files.
3. **Observation in NG-SIEM/NG-SOAR** – Confirm alerts appear in NG-SIEM/NG-SOAR and correlate with simulator activity. *Validation:* matching alert IDs and correlation graphs are visible. *Artifacts:* SIEM export files and screenshots of correlation graphs.
4. **CTI Ingestion** – Ensure trainees submit CTI data and that indicators link to SIEM alerts. *Validation:* CTI module logs show successful ingestion and linkage to alert IDs. *Artifacts:* CTI ingestion logs and mapped indicator lists.
5. **Incident Escalation** – Confirm that validated incidents were escalated via the Decide service using the provided script. *Validation:* Decide responds with `status: escalated` and the incident appears in CICMS. *Artifacts:* escalation script output and CICMS incident entry.
6. **Final Evaluation** – Review submitted reports and platform evaluation results. *Validation:* each trainee’s submission status is recorded in the grading dashboard. *Artifacts:* completed grading rubric and evaluation summaries.

These workflows ensure that trainees gain practical experience while instructors maintain oversight within the simulated environment.

## Post-Incident Reporting and Iteration

Run `subcase_1c/scripts/generate_post_incident_report.sh` once evaluations are complete to gather NG‑SIEM, BIPS and Act logs. Review the resulting file in `reports/` following the guidance in `docs/post_incident_process.md` and update playbooks or teaching materials accordingly before the next training cycle.  When the IRIS case poller processes a closed case it also tags the related MISP event, executes `scripts/update_bips_model.sh` to retrain or tune the BIPS model using the shared `subcase_1c/bips/ids_ml.py` helpers, and runs `scripts/commit_playbooks.sh` to validate and version updated CACAO playbooks (creating a Git commit when changes are present).  Results of these actions are appended to `sequence.log` for auditing, and any missing helper scripts are noted without interrupting case processing.

## Log Retrieval and Analysis

Shell commands executed on trainee and target machines are stored in `/var/log/commands.log` and forwarded to the NG‑SIEM by Filebeat. To review activity:

1. Access the NG‑SIEM dashboard (Kibana) and search the `commands` index for specific hosts or time ranges.
2. Correlate command logs with other indexes such as alerts from BIPS or NG‑SOAR to trace trainee actions and resulting events.
3. For offline review, fetch `/var/log/commands.log` from the relevant machine and analyze it with standard tools like `less`, `grep` or timeline analysis utilities.

These logs provide detailed insight into trainee behavior and support both real‑time monitoring and post‑exercise assessments.
